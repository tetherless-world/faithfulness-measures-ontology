---
layout: default
title: Faithfulness Measures Ontology
---

[Abstract](#abstract) | [Resources](#resources) | [Team](#contributors) | [Publications](#publications)

<h1 class="page-title" style="text-transform:uppercase;" id="header">Faithfulness Measures Ontology</h1>

<p class="message">A website to track the progress of the Explanation Faithfulness Measures Ontology project.</p>

<hr>
<article class="mb-5" id="abstract">
<content>
<h2>About</h2>
    <p>As artificial intelligence systems, especially large language models, gain popularity there is an increased desire to trust these systems. One common method to increase trust is to provide explanations. These explanations provide information about the system’s workings and the knowledge used in its general reasoning processes or the processes behind a specific decision [1]. However, ensuring that these explanations are faithful, that they accurately represent that reasoning process, is a difficult and ongoing task [2]. Over 60 measures have been proposed in the last 5 years to quantify how faithful an explanation is, but it is difficult to compare them to decide which one is most appropriate for a given use case. </p>
    <p>An ontology would provide a structured representation of faithfulness measures that would make comparisons easier. It would also document the inferences that researchers use to evaluate and categorize these measures. Researchers proposing new measures would be able to communicate the benefits of their idea with a shared vocabulary. Ideally it would also serve as the schema for a comprehensive knowledge graph of measures which would power a recommendation system for AI explainability researchers.</p>
    <p>Therefore this project will develop such as ontology. For this project, the measures will be limited to those used to evaluate language-task explanations. This includes both natural language and saliency explanations. Image and graph-based tasks and their explanations will not be included due to the variety in their explanations and faithfulness measures, but the ontology will be developed such that future work could expand the ontology to include these measures. The development process will follow the ontology engineer methods presented in [3] and success will be determined by whether we can answer the competency questions developed in the project’s use case.</p>
    <p>[1] Chari, S., Gruen, D. M., Seneviratne, O., & McGuinness, D. L. (2020). Foundations of explainable knowledge-enabled systems. In Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges (pp. 23-48). IOS Press.</p>
    <p>[2] Jacovi, A., & Goldberg, Y. (2020). Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?. arXiv preprint arXiv:2004.03685.</p>
    <p>[3] Kendall, E. F., & McGuinness, D. L. (2019). Ontology engineering. Morgan & Claypool Publishers.</p>
    </content> 

<hr/>
<article class="mb-5" id="resources">
<content>
<h2>List of Resources</h2>
<ul>
<table style="width:100%">
    <tr>
        <th>Resources</th>
        <th>Links</th> 
    </tr>  
    <tr>
        <td>Use Case</td>
        <td><!-- TODO ---></td> 
    </tr>
</table>
</ul>
</content>

<hr/>
<article class="mb-5" id="contributors">
<content>
<h2>Team</h2>
Danielle Villa*<sup>1</sup>, Maria Chang<sup>2</sup> , Deborah L. McGuinness<sup>1</sup>
<h3><a href="https://www.rpi.edu/"> <sup>1</sup>Rensselaer Polytechnic Institute</a> | <a href="https://research.ibm.com/science"> <sup>2</sup>IBM Research</a></h3>
<h3>*Contact at: villad4@rpi.edu </h3>
</content>

<hr/>
<article class="mb-5" id="publications">
<content>
<h2>Publications</h2>
None so far, but stay tuned!
</content>